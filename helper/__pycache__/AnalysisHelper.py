import sys
import numpy as np

from math import sqrt

from pandas import DataFrame, MultiIndex, concat, DatetimeIndex
from scipy.stats import t, pearsonr, spearmanr, shapiro, normaltest, ks_2samp, bartlett, fligner, levene, chi2_contingency, norm

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score, recall_score, precision_score, f1_score

from statsmodels.formula.api import ols, logit
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.gofplots import qqplot

from pca import pca

from matplotlib import pyplot as plt
import seaborn as sb

from tabulate import tabulate

from OlsResult import OlsResult
from LogitResult import LogitResult


class AnalysisHelper:
    """
    í†µê³„ë¶„ì„ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
    """
    def __init__(self):
        """
        ìƒì„±ì
        """
        plt.rcParams["font.family"] = 'AppleGothic' if sys.platform == 'darwin' else 'Malgun Gothic'
        plt.rcParams["font.size"] = 12
        plt.rcParams["figure.figsize"] = (15, 5)
        plt.rcParams["axes.unicode_minus"] = False

    def pretty_print(self, df, headers="keys", tablefmt="psql", numalign="right"):
        """
        ë°ì´í„° í”„ë ˆì„ì„ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥

        Args
        -------
        - df (DataFrame): ë°ì´í„° í”„ë ˆì„
        - headers (str or list, optional): í—¤ë” ëª©ë¡ Defaults to "keys".
        - tablefmt (str, optional): í…Œì´ë¸” í˜•ì‹. Defaults to "psql".
        - numalign (str, optional): ìˆ«ì í˜•ì‹ ì •ë ¬ ë°©í–¥. Defaults to "right".
        """
        print(tabulate(df, headers=headers, tablefmt=tablefmt, numalign=numalign))

    def set_datetime_index(self, df, field=None, inplace=False):
        """
        ë°ì´í„° í”„ë ˆì„ì˜ ì¸ë±ìŠ¤ë¥¼ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Parameters
        -------
        - df: ë°ì´í„° í”„ë ˆì„
        - inplace: ì›ë³¸ ë°ì´í„° í”„ë ˆì„ì— ì ìš© ì—¬ë¶€

        Returns
        -------
        - ì¸ë±ìŠ¤ê°€ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ ë°ì´í„° í”„ë ˆì„
        """
        if inplace:
            if field is not None:
                df.set_index(field, inplace=True)

            df.index = DatetimeIndex(df.index.values, freq=df.index.inferred_freq)
            df.sort_index(inplace=True)
        else:
            cdf = df.copy()

            if field is not None:
                cdf.set_index(field, inplace=True)

            cdf.index = DatetimeIndex(cdf.index.values, freq=cdf.index.inferred_freq)
            cdf.sort_index(inplace=True)
            return cdf

    def get_iq(self, field, is_print=True):
        """
        IQR(Interquartile Range)ë¥¼ ì´ìš©í•œ ì´ìƒì¹˜ ê²½ê³„ê°’ ê³„ì‚°

        Parameters
        ------- 
        - field: ë°ì´í„° í”„ë ˆì„ì˜ í•„ë“œ

        Returns
        -------
        - ê²°ì¸¡ì¹˜ê²½ê³„: ì´ìƒì¹˜ ê²½ê³„ê°’ ë¦¬ìŠ¤íŠ¸
        """
        q1 = field.quantile(q=0.25)
        q3 = field.quantile(q=0.75)
        iqr = q3 - q1
        í•˜í•œ = q1 - 1.5 * iqr
        ìƒí•œ = q3 + 1.5 * iqr
        ê·¹ë‹¨ì¹˜ê²½ê³„ = [í•˜í•œ, ìƒí•œ]

        df = DataFrame({
            "ê·¹ë‹¨ì¹˜ ê²½ê³„": [í•˜í•œ, ìƒí•œ]
        }, index=['í•˜í•œ', 'ìƒí•œ'])

        if is_print:
            self.pretty_print(df)
        else:
            return ê·¹ë‹¨ì¹˜ê²½ê³„

    def replace_outlier(self, df, fieldName):
        """
        ì´ìƒì¹˜ë¥¼ íŒë³„í•˜ì—¬ ê²°ì¸¡ì¹˜ë¡œ ì¹˜í™˜

        Parameters
        -------
        - df: ë°ì´í„° í”„ë ˆì„
        - fieldName: ì´ìƒì¹˜ë¥¼ íŒë³„í•  í•„ë“œëª…

        Returns
        -------
        - cdf : ê²°ì¸¡ì¹˜ë¥¼ ì´ìƒì¹˜ë¡œ ì¹˜í™˜í•œ ë°ì´í„° í”„ë ˆì„
        """
        cdf = df.copy()

        # fieldNameì´ Listê°€ ì•„ë‹ˆë©´ Listë¡œ ë³€í™˜
        if not isinstance(fieldName, list):
            fieldName = [fieldName]

        for f in fieldName:
            ê²°ì¸¡ì¹˜ê²½ê³„ = self.get_iq(cdf[f], is_print=False)
            cdf.loc[cdf[f] < ê²°ì¸¡ì¹˜ê²½ê³„[0], f] = np.nan
            cdf.loc[cdf[f] > ê²°ì¸¡ì¹˜ê²½ê³„[1], f] = np.nan

        return cdf

    def replace_missing_value(self, df, strategy='mean'):
        """
        ê²°ì¸¡ì¹˜ ì •ì œ

        Parameters
        -------
        - df: ë°ì´í„° í”„ë ˆì„
        - strategy: ê²°ì¸¡ì¹˜ ëŒ€ì²´ ì „ëµ(mean, median, most_frequent). ê¸°ë³¸ê°’ì€ mean

        Returns
        -------
        - re_df: ì •ì œëœ ë°ì´í„° í”„ë ˆì„
        """
        imr = SimpleImputer(missing_values=np.nan, strategy=strategy)
        df_imr = imr.fit_transform(df.values)
        re_df = DataFrame(df_imr, index=df.index, columns=df.columns)
        return re_df

    def set_category(self, df, fields=[]):
        """
        ë°ì´í„° í”„ë ˆì„ì—ì„œ ì§€ì •ëœ í•„ë“œë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€ê²½í•œë‹¤.

        Parameters
        -------
        - df: ë°ì´í„° í”„ë ˆì„
        - fields: ë²”ì£¼í˜•ìœ¼ë¡œ ë³€ê²½í•  í•„ë“œëª… ë¦¬ìŠ¤íŠ¸. ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸(ì „ì²´ í•„ë“œ ëŒ€ìƒ)

        Returns
        -------
        - cdf: ë²”ì£¼í˜•ìœ¼ë¡œ ë³€ê²½ëœ ë°ì´í„° í”„ë ˆì„
        """
        cdf = df.copy()
        # ë°ì´í„° í”„ë ˆì„ì˜ ë³€ìˆ˜ëª…ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        ilist = list(cdf.dtypes.index)
        # ë°ì´í„° í”„ë ˆì„ì˜ ë³€ìˆ˜í˜•ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        vlist = list(cdf.dtypes.values)

        # ë³€ìˆ˜í˜•ì— ëŒ€í•œ ë°˜ë³µ ì²˜ë¦¬
        for i, v in enumerate(vlist):
            # ë³€ìˆ˜í˜•ì´ objectì´ë©´?
            if v == 'object':
                # ë³€ìˆ˜ëª…ì„ ê°€ì ¸ì˜¨ë‹¤.
                field_name = ilist[i]

                # ëŒ€ìƒ í•„ë“œ ëª©ë¡ì´ ì„¤ì •ë˜ì§€ ì•Šê±°ë‚˜(ì „ì²´í•„ë“œ ëŒ€ìƒ), í˜„ì¬ í•„ë“œê°€ ëŒ€ìƒ í•„ë“œëª©ë¡ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´?
                if not fields or field_name not in fields:
                    continue

                # ê°€ì ¸ì˜¨ ë³€ìˆ˜ëª…ì— ëŒ€í•´ ê°’ì˜ ì¢…ë¥˜ë³„ë¡œ ë¹ˆë„ë¥¼ ì¹´ìš´íŠ¸ í•œ í›„ ì¸ë±ìŠ¤ ì´ë¦„ìˆœìœ¼ë¡œ ì •ë ¬
                vc = cdf[field_name].value_counts().sort_index()
                # print(vc)

                # ì¸ë±ìŠ¤ ì´ë¦„ìˆœìœ¼ë¡œ ì •ë ¬ëœ ê°’ì˜ ì¢…ë¥˜ë³„ë¡œ ë°˜ë³µ ì²˜ë¦¬
                for ii, vv in enumerate(list(vc.index)):
                    # ì¼ë ¨ë²ˆí˜¸ê°’ìœ¼ë¡œ ì¹˜í™˜
                    cdf.loc[cdf[field_name] == vv, field_name] = ii

                # í•´ë‹¹ ë³€ìˆ˜ì˜ ë°ì´í„° íƒ€ì…ì„ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜
                cdf[field_name] = cdf[field_name].astype('category')

        return cdf

    def clear_stopwords(self, nouns, stopwords_file_path="wordcloud/stopwords-ko.txt"):
        """
        ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œë‹¤.

        Parameters
        -------
        - nouns: ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
        - stopwords_file_path: ë¶ˆìš©ì–´ íŒŒì¼ ê²½ë¡œ. ê¸°ë³¸ê°’ì€ wordcloud/stopwords-ko.txt

        Returns
        -------
        - data_set: ë¶ˆìš©ì–´ê°€ ì œê±°ëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        with open(stopwords_file_path, 'r', encoding='utf-8') as f:
            stopwords = f.readlines()

            for i, v in enumerate(stopwords):
                stopwords[i] = v.strip()

        data_set = []

        for v in nouns:
            if v not in stopwords:
                data_set.append(v)

        return data_set

    def get_confidence_interval(self, data, clevel=0.95, is_print=True):
        """
        ì‹ ë¢°êµ¬ê°„ ê³„ì‚°

        Parameters
        -------
        - data: ë°ì´í„°
        - clevel: ì‹ ë¢°ìˆ˜ì¤€. ê¸°ë³¸ê°’ì€ 0.95

        Returns
        -------
        - cmin: ì‹ ë¢°êµ¬ê°„ í•˜í•œ
        - cmax: ì‹ ë¢°êµ¬ê°„ ìƒí•œ
        """
        n = len(data)                           # ìƒ˜í”Œ ì‚¬ì´ì¦ˆ
        dof = n - 1                             # ììœ ë„
        sample_mean = data.mean()               # í‘œë³¸ í‰ê· 
        sample_std = data.std(ddof=1)           # í‘œë³¸ í‘œì¤€ í¸ì°¨
        sample_std_error = sample_std / sqrt(n)  # í‘œë³¸ í‘œì¤€ì˜¤ì°¨

        # ì‹ ë¢°êµ¬ê°„
        cmin, cmax = t.interval(
            clevel, dof, loc=sample_mean, scale=sample_std_error)

        if is_print:
            df = DataFrame({
                "ì‹ ë¢°êµ¬ê°„": [cmin, cmax]
            }, index=['í•˜í•œ', 'ìƒí•œ'])

            self.pretty_print(df)
        else:
            return (cmin, cmax)

    def normality_test(self, *any, is_print=True):
        """
        ë¶„ì‚°ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì •ê·œì„±ì„ ê²€ì • í•œë‹¤.

        Parameters
        -------
        - any: í•„ë“œë“¤

        Returns
        -------
        - df: ê²€ì • ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
        """
        names = []

        result = {
            'field': [],
            'test': [],
            'statistic': [],
            'p-value': [],
            'result': []
        }

        for i in any:
            s, p = shapiro(i)
            result['field'].append(i.name)
            result['test'].append('shapiro')
            result['statistic'].append(s)
            result['p-value'].append(p)
            result['result'].append(p > 0.05)
            names.append('ì •ê·œì„±')

        for i in any:
            s, p = normaltest(i)
            result['field'].append(i.name)
            result['test'].append('shapiro')
            result['statistic'].append(s)
            result['p-value'].append(p)
            result['result'].append(p > 0.05)
            names.append('ì •ê·œì„±')

        n = len(any)

        for i in range(0, n):
            j = i + 1 if i < n - 1 else 0

            s, p = ks_2samp(any[i], any[j])
            result['field'].append(f'{any[i].name} vs {any[j].name}')
            result['test'].append('ks_2samp')
            result['statistic'].append(s)
            result['p-value'].append(p)
            result['result'].append(p > 0.05)
            names.append('ì •ê·œì„±')

        rdf = DataFrame(result, index=names)

        if is_print:
            self.pretty_print(rdf)
        else:
            return rdf

    def equal_variance_test(self, *any, is_print=True):
        """
        ë¶„ì‚°ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë“±ë¶„ì‚°ì„±ì„ ê²€ì • í•œë‹¤.

        Parameters
        -------
        - any: í•„ë“œë“¤

        Returns
        -------
        - df: ê²€ì • ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
        """
        s1, p1 = bartlett(*any)
        s2, p2 = fligner(*any)
        s3, p3 = levene(*any)

        names = []

        for i in any:
            names.append(i.name)

        fix = " vs "
        name = fix.join(names)
        index = ['ë“±ë¶„ì‚°ì„±', 'ë“±ë¶„ì‚°ì„±', 'ë“±ë¶„ì‚°ì„±']

        df = DataFrame({
            'field': [name, name, name],
            'test': ['Bartlett', 'Fligner', 'Levene'],
            'statistic': [s1, s2, s3],
            'p-value': [p1, p2, p3],
            'result': [p1 > 0.05, p2 > 0.05, p3 > 0.05]
        }, index=index)

        if is_print:
            self.pretty_print(df)
        else:
            return df

    def independence_test(self, *any, is_print=True):
        """
        ë¶„ì‚°ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë…ë¦½ì„±ì„ ê²€ì •í•œë‹¤.

        Parameters
        -------
        - any: í•„ë“œë“¤

        Returns
        -------
        - df: ê²€ì • ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
        """
        df = DataFrame(any).T
        result = chi2_contingency(df)

        names = []

        for i in any:
            names.append(i.name)

        fix = " vs "
        name = fix.join(names)

        index = ['ë…ë¦½ì„±']

        df = DataFrame({
            'field': [name],
            'test': ['Chi2'],
            'statistic': [result.statistic],
            'p-value': [result.pvalue],
            'result': [result.pvalue > 0.05]
        }, index=index)

        if is_print:
            self.pretty_print(df)
        else:
            return df

    def all_test(self, *any, is_print=True):
        """
        ì •ê·œì„±, ë“±ë¶„ì‚°ì„±, ë…ë¦½ì„±ì„ ëª¨ë‘ ê²€ì •í•œë‹¤.

        Parameters
        -------
        - any: í•„ë“œë“¤

        Returns
        -------
        - df: ê²€ì • ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
        """
        normality_test = self.normality_test(*any, is_print=False)
        equal_variance_test = self.equal_variance_test(*any, is_print=False)
        independence_test = self.independence_test(*any, is_print=False)
        
        cc = concat([normality_test, equal_variance_test, independence_test])

        if is_print:
            self.pretty_print(cc)
        else:
            return cc

    def pearson_r(self, df, is_print=True):
        """
        í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒê´€ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.

        Parameters
        -------
        - df: ë°ì´í„° í”„ë ˆì„

        Returns
        -------
        - rdf: ìƒê´€ë¶„ì„ ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
        """
        names = df.columns
        n = len(names)
        pv = 0.05

        data = []

        for i in range(0, n):
            # ê¸°ë³¸ì ìœ¼ë¡œ i ë‹¤ìŒ ìœ„ì¹˜ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ iê°€ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ì¼ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
            j = i + 1 if i < n - 1 else 0

            fields = names[i] + ' vs ' + names[j]
            s, p = pearsonr(df[names[i]], df[names[j]])
            result = p < pv

            data.append({'fields': fields, 'statistic': s,
                        'pvalue': p, 'result': result})

        rdf = DataFrame(data)
        rdf.set_index('fields', inplace=True)

        if is_print:
            self.pretty_print(rdf)
        else:
            return rdf

    def spearman_r(self, df, is_print=True):
        """
        ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒê´€ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.

        Parameters
        -------
        - df: ë°ì´í„° í”„ë ˆì„

        Returns
        -------
        - rdf: ìƒê´€ë¶„ì„ ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
        """
        names = df.columns
        n = len(names)
        pv = 0.05

        data = []

        for i in range(0, n):
            # ê¸°ë³¸ì ìœ¼ë¡œ i ë‹¤ìŒ ìœ„ì¹˜ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ iê°€ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ì¼ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
            j = i + 1 if i < n - 1 else 0

            fields = names[i] + ' vs ' + names[j]
            s, p = spearmanr(df[names[i]], df[names[j]])
            result = p < pv

            data.append({'fields': fields, 'statistic': s,
                        'pvalue': p, 'result': result})

        rdf = DataFrame(data)
        rdf.set_index('fields', inplace=True)

        if is_print:
            self.pretty_print(rdf)
        else:
            return rdf

    def scalling(self, df, yname=None):
        """
        ë°ì´í„° í”„ë ˆì„ì„ í‘œì¤€í™” í•œë‹¤.

        Parameters
        -------
        - df: ë°ì´í„° í”„ë ˆì„
        - yname: ì¢…ì†ë³€ìˆ˜ ì´ë¦„

        Returns
        -------
        - x_train_std_df: í‘œì¤€í™”ëœ ë…ë¦½ë³€ìˆ˜ ë°ì´í„° í”„ë ˆì„
        - y_train_std_df: í‘œì¤€í™”ëœ ì¢…ì†ë³€ìˆ˜ ë°ì´í„° í”„ë ˆì„
        """
        # í‰ì†Œì—ëŠ” ynameì„ ì œê±°í•œ í•­ëª©ì„ ì‚¬ìš©
        # ynameì´ ìˆì§€ ì•Šë‹¤ë©´ dfë¥¼ ë³µì‚¬
        x_train = df.drop([yname], axis=1) if yname else df.copy()
        x_train_std = StandardScaler().fit_transform(x_train)
        x_train_std_df = DataFrame(x_train_std, columns=x_train.columns)

        if yname:
            y_train = df.filter([yname])
            y_train_std = StandardScaler().fit_transform(y_train)
            y_train_std_df = DataFrame(y_train_std, columns=y_train.columns)

        if yname:
            result = (x_train_std_df, y_train_std_df)
        else:
            result = x_train_std_df

        return result

    def get_best_features(self, x_train_std_df):
        pca_model = pca()
        fit = pca_model.fit_transform(x_train_std_df)
        topfeat_df = fit['topfeat']

        best = topfeat_df.query("type=='best'")
        feature = list(set(list(best['feature'])))

        return (feature, topfeat_df)

    def ols(self, data, y, x):
        """
        íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.

        Parameters
        -------
        - data : ë°ì´í„° í”„ë ˆì„
        - y: ì¢…ì†ë³€ìˆ˜ ì´ë¦„
        - x: ë…ë¦½ë³€ìˆ˜ì˜ ì´ë¦„ë“¤(ë¦¬ìŠ¤íŠ¸)
        """

        # ë…ë¦½ë³€ìˆ˜ì˜ ì´ë¦„ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if type(x) != list:
            x = [x]

        # ì¢…ì†ë³€ìˆ˜~ë…ë¦½ë³€ìˆ˜1+ë…ë¦½ë³€ìˆ˜2+ë…ë¦½ë³€ìˆ˜3+... í˜•íƒœì˜ ì‹ì„ ìƒì„±
        expr = "%s~%s" % (y, "+".join(x))

        # íšŒê·€ëª¨ë¸ ìƒì„±
        model = ols(expr, data=data)
        # ë¶„ì„ ìˆ˜í–‰
        fit = model.fit()

        # íŒŒì´ì¬ ë¶„ì„ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥í•œë‹¤.
        summary = fit.summary()

        # ì²« ë²ˆì§¸, ì„¸ ë²ˆì§¸ í‘œì˜ ë‚´ìš©ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë¶„í•´
        my = {}

        for k in range(0, 3, 2):
            items = summary.tables[k].data
            # print(items)

            for item in items:
                # print(item)
                n = len(item)

                for i in range(0, n, 2):
                    key = item[i].strip()[:-1]
                    value = item[i+1].strip()

                    if key and value:
                        my[key] = value

        # ë‘ ë²ˆì§¸ í‘œì˜ ë‚´ìš©ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë¶„í•´í•˜ì—¬ myì— ì¶”ê°€
        my['variables'] = []
        name_list = list(data.columns)
        # print(name_list)

        for i, v in enumerate(summary.tables[1].data):
            if i == 0:
                continue

            # ë³€ìˆ˜ì˜ ì´ë¦„
            name = v[0].strip()

            vif = 0

            # InterceptëŠ” ì œì™¸
            if name in name_list:
                # ë³€ìˆ˜ì˜ ì´ë¦„ ëª©ë¡ì—ì„œ í˜„ì¬ ë³€ìˆ˜ê°€ ëª‡ ë²ˆì§¸ í•­ëª©ì¸ì§€ ì°¾ê¸°
                j = name_list.index(name)
                vif = variance_inflation_factor(data, j)

            my['variables'].append({
                "name": name,
                "coef": v[1].strip(),
                "std err": v[2].strip(),
                "t": v[3].strip(),
                "P-value": v[4].strip(),
                "Beta": 0,
                "VIF": vif,
            })

        # ê²°ê³¼í‘œë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ êµ¬ì„±
        mylist = []
        yname_list = []
        xname_list = []

        for i in my['variables']:
            if i['name'] == 'Intercept':
                continue

            yname_list.append(y)
            xname_list.append(i['name'])

            item = {
                "B": i['coef'],
                "í‘œì¤€ì˜¤ì°¨": i['std err'],
                "Î²": i['Beta'],
                "t": "%s*" % i['t'],
                "ìœ ì˜í™•ë¥ ": i['P-value'],
                "VIF": i["VIF"]
            }

            mylist.append(item)

        table = DataFrame(mylist,
                          index=MultiIndex.from_arrays([yname_list, xname_list], names=['ì¢…ì†ë³€ìˆ˜', 'ë…ë¦½ë³€ìˆ˜']))

        # ë¶„ì„ê²°ê³¼
        result = "ğ‘…(%s), ğ‘…^2(%s), ğ¹(%s), ìœ ì˜í™•ë¥ (%s), Durbin-Watson(%s)" % (
            my['R-squared'], my['Adj. R-squared'], my['F-statistic'], my['Prob (F-statistic)'], my['Durbin-Watson'])

        # ëª¨í˜• ì í•©ë„ ë³´ê³ 
        goodness = "%sì— ëŒ€í•˜ì—¬ %së¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ %s(F(%s,%s) = %s, p < 0.05)." % (y, ",".join(x), "ìœ ì˜í•˜ë‹¤" if float(
            my['Prob (F-statistic)']) < 0.05 else "ìœ ì˜í•˜ì§€ ì•Šë‹¤", my['Df Model'], my['Df Residuals'], my['F-statistic'])

        # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
        varstr = []

        for i, v in enumerate(my['variables']):
            if i == 0:
                continue

            s = "%sì˜ íšŒê·€ê³„ìˆ˜ëŠ” %s(p%s0.05)ë¡œ, %sì— ëŒ€í•˜ì—¬ %s."
            k = s % (v['name'], v['coef'], "<" if float(v['P-value']) < 0.05 else '>', y,
                     'ìœ ì˜ë¯¸í•œ ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤' if float(v['P-value']) < 0.05 else 'ìœ ì˜í•˜ì§€ ì•Šì€ ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤')

            varstr.append(k)

        ols_result = OlsResult()
        ols_result.model = model
        ols_result.fit = fit
        ols_result.summary = summary
        ols_result.table = table
        ols_result.result = result
        ols_result.goodness = goodness
        ols_result.varstr = varstr

        return ols_result

    def logit(self, data, y, x, subset=None):
        """
        ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.

        Parameters
        -------
        - data : ë°ì´í„° í”„ë ˆì„
        - y: ì¢…ì†ë³€ìˆ˜ ì´ë¦„
        - x: ë…ë¦½ë³€ìˆ˜ì˜ ì´ë¦„ë“¤(ë¦¬ìŠ¤íŠ¸)
        """

        # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
        df = data.copy()

        # ë…ë¦½ë³€ìˆ˜ì˜ ì´ë¦„ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if type(x) != list:
            x = [x]

        # ì¢…ì†ë³€ìˆ˜~ë…ë¦½ë³€ìˆ˜1+ë…ë¦½ë³€ìˆ˜2+ë…ë¦½ë³€ìˆ˜3+... í˜•íƒœì˜ ì‹ì„ ìƒì„±
        expr = "%s~%s" % (y, "+".join(x))

        # íšŒê·€ëª¨ë¸ ìƒì„±
        model = logit(expr, data=df)
        # ë¶„ì„ ìˆ˜í–‰
        fit = model.fit()

        # íŒŒì´ì¬ ë¶„ì„ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥í•œë‹¤.
        summary = fit.summary()

        # ì˜ì‚¬ê²°ì •ê³„ìˆ˜
        prs = fit.prsquared

        # ì˜ˆì¸¡ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
        df['ì˜ˆì¸¡ê°’'] = fit.predict(df.drop([y], axis=1))
        df['ì˜ˆì¸¡ê²°ê³¼'] = df['ì˜ˆì¸¡ê°’'] > 0.5

        # í˜¼ë™í–‰ë ¬
        cm = confusion_matrix(df[y], df['ì˜ˆì¸¡ê²°ê³¼'])
        tn, fp, fn, tp = cm.ravel()
        cmdf = DataFrame([[tn, tp], [fn, fp]], index=[
                         'True', 'False'], columns=['Negative', 'Positive'])

        # RAS
        ras = roc_auc_score(df[y], df['ì˜ˆì¸¡ê²°ê³¼'])

        # ìœ„ì–‘ì„±ìœ¨, ì¬í˜„ìœ¨, ì„ê³„ê°’(ì‚¬ìš©ì•ˆí•¨)
        fpr, tpr, thresholds = roc_curve(df[y], df['ì˜ˆì¸¡ê²°ê³¼'])

        # ì •í™•ë„
        acc = accuracy_score(df[y], df['ì˜ˆì¸¡ê²°ê³¼'])

        # ì •ë°€ë„
        pre = precision_score(df[y], df['ì˜ˆì¸¡ê²°ê³¼'])

        # ì¬í˜„ìœ¨
        recall = recall_score(df[y], df['ì˜ˆì¸¡ê²°ê³¼'])

        # F1 score
        f1 = f1_score(df[y], df['ì˜ˆì¸¡ê²°ê³¼'])

        # ìœ„ì–‘ì„±ìœ¨
        fallout = fp / (fp + tn)

        # íŠ¹ì´ì„±
        spe = 1 - fallout

        result_df = DataFrame({'ì„¤ëª…ë ¥(Pseudo-Rsqe)': [fit.prsquared], 'ì •í™•ë„(Accuracy)': [acc], 'ì •ë°€ë„(Precision)': [pre], 'ì¬í˜„ìœ¨(Recall, TPR)': [
                              recall], 'ìœ„ì–‘ì„±ìœ¨(Fallout, FPR)': [fallout], 'íŠ¹ì´ì„±(Specificity, TNR)': [spe], 'RAS': [ras], 'f1_score': [f1]})

        # ì˜¤ì¦ˆë¹„
        coef = fit.params
        odds_rate = np.exp(coef)
        odds_rate_df = DataFrame(odds_rate, columns=['odds_rate'])

        logit_result = LogitResult()
        logit_result.model = model
        logit_result.fit = fit
        logit_result.summary = summary
        logit_result.prs = prs
        logit_result.cmdf = cmdf
        logit_result.result_df = result_df
        logit_result.odds_rate_df = odds_rate_df

        return logit_result

    def exp_time_data(self, data, yname, sd_model="m", max_diff=1):
        plt.rcParams["font.family"] = 'AppleGothic' if sys.platform == 'darwin' else 'Malgun Gothic'
        plt.rcParams["font.size"] = 12
        plt.rcParams["axes.unicode_minus"] = False

        df = data.copy()

        # ë°ì´í„° ì •ìƒì„± ì—¬ë¶€
        stationarity = False

        # ë°˜ë³µ ìˆ˜í–‰ íšŸìˆ˜
        count = 0

        # ê²°ì¸¡ì¹˜ ì¡´ì¬ ì—¬ë¶€
        na_count = df[yname].isna().sum()
        print("ê²°ì¸¡ì¹˜ ìˆ˜: %d" % na_count)

        plt.figure(figsize=(4, 5))
        sb.boxplot(data=df, y=yname)
        plt.show()
        plt.close()

        # ì‹œê³„ì—´ ë¶„í•´
        model_name = 'multiplicative' if sd_model == 'm' else 'additive'
        sd = seasonal_decompose(df[yname], model=model_name)

        figure = sd.plot()
        figure.set_figwidth(15)
        figure.set_figheight(16)
        fig, ax1, ax2, ax3, ax4 = figure.get_children()
        figure.subplots_adjust(hspace=0.4)

        ax1.set_ylabel("Original")
        ax1.grid(True)
        ax1.title.set_text("Original")
        ax2.grid(True)
        ax2.title.set_text("Trend")
        ax3.grid(True)
        ax3.title.set_text("Seasonal")
        ax4.grid(True)
        ax4.title.set_text("Residual")

        plt.show()

        # ACF, PACF ê²€ì •
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))
        fig.subplots_adjust(hspace=0.4)

        sb.lineplot(data=df, x=df.index, y=yname, ax=ax1)
        ax1.title.set_text("Original")

        plot_acf(df[yname], ax=ax2)
        ax2.title.set_text("ACF Test")

        plot_pacf(df[yname], ax=ax3)
        ax3.title.set_text("PACF Test")

        plt.show()
        plt.close()

        while not stationarity:
            if count == 0:
                print("=========== ì›ë³¸ ë°ì´í„° ===========")
            else:
                print("=========== %dì°¨ ì°¨ë¶„ ë°ì´í„° ===========" % count)

            # ADF Test
            ar = adfuller(df[yname])

            ardict = {
                'ê²€ì •í†µê³„ëŸ‰(ADF Statistic)': [ar[0]],
                'ìœ ì˜ìˆ˜ì¤€(p-value)': [ar[1]],
                'ìµœì ì°¨ìˆ˜(num of lags)': [ar[2]],
                'ê´€ì¸¡ì¹˜ ê°œìˆ˜(num of observations)': [ar[3]]
            }

            for key, value in ar[4].items():
                ardict['ê¸°ê°ê°’(Critical Values) %s' % key] = value

            stationarity = ar[1] < 0.05
            ardict['ë°ì´í„° ì •ìƒì„± ì—¬ë¶€(0=Flase,1=True)'] = stationarity

            ardf = DataFrame(ardict, index=['ADF Test']).T

            print(tabulate(ardf, headers=["ADF Test", ""], tablefmt='psql', numalign="right"))

            # ì°¨ë¶„ ìˆ˜í–‰
            df = df.diff().dropna()

            # ë°˜ë³µì„ ê³„ì†í• ì§€ ì—¬ë¶€ íŒë‹¨
            count += 1
            if count == max_diff:
                break

    def arima_diagnostics(self, resids, n_lags=40):

        # create placeholder subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))

        r = resids
        resids = (r - np.nanmean(r)) / np.nanstd(r)
        resids_nonmissing = resids[~(np.isnan(resids))]

        # residuals over time
        sb.lineplot(x=np.arange(len(resids)), y=resids, ax=ax1)
        ax1.set_title('Standardized residuals')
        ax1.grid()
        ax1.set_xlim(ax1.get_xlim())
        sb.lineplot(x=ax1.get_xlim(), y=[0, 0], linestyle='--', color='red', alpha=0.7, ax=ax1)

        # distribution of residuals
        x_lim = (-1.96 * 2, 1.96 * 2)
        r_range = np.linspace(x_lim[0], x_lim[1])
        norm_pdf = norm.pdf(r_range)

        sb.histplot(resids_nonmissing, kde=True, ax=ax2)
        ax2.plot(r_range, norm_pdf, 'g', lw=2, label='N(0,1)')
        ax2.set_title('Distribution of standardized residuals')
        ax2.set_xlim(x_lim)
        ax2.legend()
        ax2.grid()

        # Q-Q plot
        qq = qqplot(resids_nonmissing, line='s', ax=ax3)
        ax3.set_title('Q-Q plot')
        ax3.grid()

        # ACF plot
        plot_acf(resids, ax=ax4, alpha=0.05)
        ax4.set_title('ACF plot')
        ax4.grid()

        plt.show()
        plt.close()


helper = AnalysisHelper()
