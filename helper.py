"""
í†µê³„ë¶„ì„ ìœ í‹¸ë¦¬í‹°
@Author: ì´ê´‘í˜¸(leekh4232@gmail.com)
"""
import numpy as np
from pandas import DataFrame, MultiIndex, concat, merge
from math import sqrt
from scipy.stats import t, pearsonr, spearmanr
from sklearn.impute import SimpleImputer
from scipy.stats import shapiro, normaltest, ks_2samp, bartlett, fligner, levene, chi2_contingency
from statsmodels.formula.api import ols
import re
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler
from pca import pca


def getIq(field):
    """
    IQR(Interquartile Range)ë¥¼ ì´ìš©í•œ ì´ìƒì¹˜ ê²½ê³„ê°’ ê³„ì‚°

    Parameters
    ------- 
    - field: ë°ì´í„° í”„ë ˆì„ì˜ í•„ë“œ

    Returns
    -------
    - ê²°ì¸¡ì¹˜ê²½ê³„: ì´ìƒì¹˜ ê²½ê³„ê°’ ë¦¬ìŠ¤íŠ¸
    """
    q1 = field.quantile(q=0.25)
    q3 = field.quantile(q=0.75)
    iqr = q3 - q1
    í•˜í•œ = q1 - 1.5 * iqr
    ìƒí•œ = q3 + 1.5 * iqr
    ê²°ì¸¡ì¹˜ê²½ê³„ = [í•˜í•œ, ìƒí•œ]
    return ê²°ì¸¡ì¹˜ê²½ê³„


def replaceOutlier(df, fieldName):
    """
    ì´ìƒì¹˜ë¥¼ íŒë³„í•˜ì—¬ ê²°ì¸¡ì¹˜ë¡œ ì¹˜í™˜

    Parameters
    -------
    - df: ë°ì´í„° í”„ë ˆì„
    - fieldName: ì´ìƒì¹˜ë¥¼ íŒë³„í•  í•„ë“œëª…

    Returns
    -------
    - cdf : ê²°ì¸¡ì¹˜ë¥¼ ì´ìƒì¹˜ë¡œ ì¹˜í™˜í•œ ë°ì´í„° í”„ë ˆì„
    """
    cdf = df.copy()

    # fieldNameì´ Listê°€ ì•„ë‹ˆë©´ Listë¡œ ë³€í™˜
    if not isinstance(fieldName, list):
        fieldName = [fieldName]

    for f in fieldName:
        ê²°ì¸¡ì¹˜ê²½ê³„ = getIq(cdf[f])
        cdf.loc[cdf[f] < ê²°ì¸¡ì¹˜ê²½ê³„[0], f] = np.nan
        cdf.loc[cdf[f] > ê²°ì¸¡ì¹˜ê²½ê³„[1], f] = np.nan

    return cdf


def replaceMissingValue(df, strategy='mean'):
    """
    ê²°ì¸¡ì¹˜ ì •ì œ

    Parameters
    -------
    - df: ë°ì´í„° í”„ë ˆì„
    - strategy: ê²°ì¸¡ì¹˜ ëŒ€ì²´ ì „ëµ(mean, median, most_frequent). ê¸°ë³¸ê°’ì€ mean

    Returns
    -------
    - re_df: ì •ì œëœ ë°ì´í„° í”„ë ˆì„
    """
    imr = SimpleImputer(missing_values=np.nan, strategy=strategy)
    df_imr = imr.fit_transform(df.values)
    re_df = DataFrame(df_imr, index=df.index, columns=df.columns)
    return re_df


def setCategory(df, fields=[]):
    """
    ë°ì´í„° í”„ë ˆì„ì—ì„œ ì§€ì •ëœ í•„ë“œë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€ê²½í•œë‹¤.

    Parameters
    -------
    - df: ë°ì´í„° í”„ë ˆì„
    - fields: ë²”ì£¼í˜•ìœ¼ë¡œ ë³€ê²½í•  í•„ë“œëª… ë¦¬ìŠ¤íŠ¸. ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸(ì „ì²´ í•„ë“œ ëŒ€ìƒ)

    Returns
    -------
    - cdf: ë²”ì£¼í˜•ìœ¼ë¡œ ë³€ê²½ëœ ë°ì´í„° í”„ë ˆì„
    """
    cdf = df.copy()
    # ë°ì´í„° í”„ë ˆì„ì˜ ë³€ìˆ˜ëª…ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    ilist = list(cdf.dtypes.index)
    # ë°ì´í„° í”„ë ˆì„ì˜ ë³€ìˆ˜í˜•ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    vlist = list(cdf.dtypes.values)

    # ë³€ìˆ˜í˜•ì— ëŒ€í•œ ë°˜ë³µ ì²˜ë¦¬
    for i, v in enumerate(vlist):
        # ë³€ìˆ˜í˜•ì´ objectì´ë©´?
        if v == 'object':
            # ë³€ìˆ˜ëª…ì„ ê°€ì ¸ì˜¨ë‹¤.
            field_name = ilist[i]

            # ëŒ€ìƒ í•„ë“œ ëª©ë¡ì´ ì„¤ì •ë˜ì§€ ì•Šê±°ë‚˜(ì „ì²´í•„ë“œ ëŒ€ìƒ), í˜„ì¬ í•„ë“œê°€ ëŒ€ìƒ í•„ë“œëª©ë¡ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´?
            if not fields or field_name not in fields:
                continue

            # ê°€ì ¸ì˜¨ ë³€ìˆ˜ëª…ì— ëŒ€í•´ ê°’ì˜ ì¢…ë¥˜ë³„ë¡œ ë¹ˆë„ë¥¼ ì¹´ìš´íŠ¸ í•œ í›„ ì¸ë±ìŠ¤ ì´ë¦„ìˆœìœ¼ë¡œ ì •ë ¬
            vc = cdf[field_name].value_counts().sort_index()
            # print(vc)

            # ì¸ë±ìŠ¤ ì´ë¦„ìˆœìœ¼ë¡œ ì •ë ¬ëœ ê°’ì˜ ì¢…ë¥˜ë³„ë¡œ ë°˜ë³µ ì²˜ë¦¬
            for ii, vv in enumerate(list(vc.index)):
                # ì¼ë ¨ë²ˆí˜¸ê°’ ìƒì„±
                vnum = ii + 1
                # print(vv, " -->", vnum)

                # ì¼ë ¨ë²ˆí˜¸ê°’ìœ¼ë¡œ ì¹˜í™˜
                cdf.loc[cdf[field_name] == vv, field_name] = vnum

            # í•´ë‹¹ ë³€ìˆ˜ì˜ ë°ì´í„° íƒ€ì…ì„ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜
            cdf[field_name] = cdf[field_name].astype('category')

    return cdf


def clearStopwords(nouns, stopwords_file_path="wordcloud/stopwords-ko.txt"):
    """
    ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œë‹¤.

    Parameters
    -------
    - nouns: ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
    - stopwords_file_path: ë¶ˆìš©ì–´ íŒŒì¼ ê²½ë¡œ. ê¸°ë³¸ê°’ì€ wordcloud/stopwords-ko.txt

    Returns
    -------
    - data_set: ë¶ˆìš©ì–´ê°€ ì œê±°ëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    with open(stopwords_file_path, 'r', encoding='utf-8') as f:
        stopwords = f.readlines()

        for i, v in enumerate(stopwords):
            stopwords[i] = v.strip()

    data_set = []

    for v in nouns:
        if v not in stopwords:
            data_set.append(v)

    return data_set


def get_confidence_interval(data, clevel=0.95):
    """
    ì‹ ë¢°êµ¬ê°„ ê³„ì‚°

    Parameters
    -------
    - data: ë°ì´í„°
    - clevel: ì‹ ë¢°ìˆ˜ì¤€. ê¸°ë³¸ê°’ì€ 0.95

    Returns
    -------
    - cmin: ì‹ ë¢°êµ¬ê°„ í•˜í•œ
    - cmax: ì‹ ë¢°êµ¬ê°„ ìƒí•œ
    """
    n = len(data)                           # ìƒ˜í”Œ ì‚¬ì´ì¦ˆ
    dof = n - 1                             # ììœ ë„
    sample_mean = data.mean()               # í‘œë³¸ í‰ê· 
    sample_std = data.std(ddof=1)           # í‘œë³¸ í‘œì¤€ í¸ì°¨
    sample_std_error = sample_std / sqrt(n)  # í‘œë³¸ í‘œì¤€ì˜¤ì°¨

    # ì‹ ë¢°êµ¬ê°„
    cmin, cmax = t.interval(
        clevel, dof, loc=sample_mean, scale=sample_std_error)

    return (cmin, cmax)


def normality_test(*any):
    """
    ë¶„ì‚°ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì •ê·œì„±ì„ ê²€ì • í•œë‹¤.

    Parameters
    -------
    - any: í•„ë“œë“¤

    Returns
    -------
    - df: ê²€ì • ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
    """
    names = []

    result = {
        'statistic': [],
        'p-value': [],
        'result': []
    }
    for i in any:
        s, p = shapiro(i)
        result['statistic'].append(s)
        result['p-value'].append(p)
        result['result'].append(p > 0.05)
        names.append(('ì •ê·œì„±', 'shapiro', i.name))

    for i in any:
        s, p = normaltest(i)
        result['statistic'].append(s)
        result['p-value'].append(p)
        result['result'].append(p > 0.05)
        names.append(('ì •ê·œì„±', 'normaltest', i.name))

    n = len(any)

    for i in range(0, n):
        j = i + 1 if i < n - 1 else 0

        s, p = ks_2samp(any[i], any[j])
        result['statistic'].append(s)
        result['p-value'].append(p)
        result['result'].append(p > 0.05)
        names.append(('ì •ê·œì„±', 'ks_2samp', f'{any[i].name} vs {any[j].name}'))

    return DataFrame(result, index=MultiIndex.from_tuples(names, names=['condition', 'test', 'field']))


def equal_variance_test(*any):
    """
    ë¶„ì‚°ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë“±ë¶„ì‚°ì„±ì„ ê²€ì • í•œë‹¤.

    Parameters
    -------
    - any: í•„ë“œë“¤

    Returns
    -------
    - df: ê²€ì • ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
    """
    s1, p1 = bartlett(*any)
    s2, p2 = fligner(*any)
    s3, p3 = levene(*any)

    names = []

    for i in any:
        names.append(i.name)

    fix = " vs "
    name = fix.join(names)
    index = [['ë“±ë¶„ì‚°ì„±', 'Bartlett', name], [
        'ë“±ë¶„ì‚°ì„±', 'Fligner', name], ['ë“±ë¶„ì‚°ì„±', 'Levene', name]]

    df = DataFrame({
        'statistic': [s1, s2, s3],
        'p-value': [p1, p2, p3],
        'result': [p1 > 0.05, p2 > 0.05, p3 > 0.05]
    }, index=MultiIndex.from_tuples(index, names=['condition', 'test', 'field']))

    return df


def independence_test(*any):
    """
    ë¶„ì‚°ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë…ë¦½ì„±ì„ ê²€ì •í•œë‹¤.

    Parameters
    -------
    - any: í•„ë“œë“¤

    Returns
    -------
    - df: ê²€ì • ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
    """
    df = DataFrame(any).T
    result = chi2_contingency(df)

    names = []

    for i in any:
        names.append(i.name)

    fix = " vs "
    name = fix.join(names)

    index = [['ë…ë¦½ì„±', 'Chi2', name]]

    df = DataFrame({
        'statistic': [result.statistic],
        'p-value': [result.pvalue],
        'result': [result.pvalue > 0.05]
    }, index=MultiIndex.from_tuples(index, names=['condition', 'test', 'field']))

    return df


def all_test(*any):
    """
    ì •ê·œì„±, ë“±ë¶„ì‚°ì„±, ë…ë¦½ì„±ì„ ëª¨ë‘ ê²€ì •í•œë‹¤.

    Parameters
    -------
    - any: í•„ë“œë“¤

    Returns
    -------
    - df: ê²€ì • ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
    """
    return concat([normality_test(*any), equal_variance_test(*any), independence_test(*any)])


def pearson_r(df):
    """
    í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒê´€ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.

    Parameters
    -------
    - df: ë°ì´í„° í”„ë ˆì„

    Returns
    -------
    - rdf: ìƒê´€ë¶„ì„ ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
    """
    names = df.columns
    n = len(names)
    pv = 0.05

    data = []

    for i in range(0, n):
        # ê¸°ë³¸ì ìœ¼ë¡œ i ë‹¤ìŒ ìœ„ì¹˜ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ iê°€ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ì¼ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
        j = i + 1 if i < n - 1 else 0

        fields = names[i] + ' vs ' + names[j]
        s, p = pearsonr(df[names[i]], df[names[j]])
        result = p < pv

        data.append({'fields': fields, 'statistic': s,
                    'pvalue': p, 'result': result})

    rdf = DataFrame(data)
    rdf.set_index('fields', inplace=True)

    return rdf


def spearman_r(df):
    """
    ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒê´€ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.

    Parameters
    -------
    - df: ë°ì´í„° í”„ë ˆì„

    Returns
    -------
    - rdf: ìƒê´€ë¶„ì„ ê²°ê³¼ ë°ì´í„° í”„ë ˆì„
    """
    names = df.columns
    n = len(names)
    pv = 0.05

    data = []

    for i in range(0, n):
        # ê¸°ë³¸ì ìœ¼ë¡œ i ë‹¤ìŒ ìœ„ì¹˜ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ iê°€ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ì¼ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
        j = i + 1 if i < n - 1 else 0

        fields = names[i] + ' vs ' + names[j]
        s, p = spearmanr(df[names[i]], df[names[j]])
        result = p < pv

        data.append({'fields': fields, 'statistic': s,
                    'pvalue': p, 'result': result})

    rdf = DataFrame(data)
    rdf.set_index('fields', inplace=True)

    return rdf

def ext_ols(data, y, x):
    """
    íšŒê·€ë¶„ì„ì„ ìˆ˜í•´í•œë‹¤.

    Parameters
    -------
    - data : ë°ì´í„° í”„ë ˆì„
    - y: ì¢…ì†ë³€ìˆ˜ ì´ë¦„
    - x: ë…ë¦½ë³€ìˆ˜ì˜ ì´ë¦„ë“¤(ë¦¬ìŠ¤íŠ¸)
    """

    # ë…ë¦½ë³€ìˆ˜ì˜ ì´ë¦„ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if type(x) != list:
        x = [x]

    # ì¢…ì†ë³€ìˆ˜~ë…ë¦½ë³€ìˆ˜1+ë…ë¦½ë³€ìˆ˜2+ë…ë¦½ë³€ìˆ˜3+... í˜•íƒœì˜ ì‹ì„ ìƒì„±
    expr = "%s~%s" % (y, "+".join(x))

    # íšŒê·€ëª¨ë¸ ìƒì„±
    model = ols(expr, data=data)
    # ë¶„ì„ ìˆ˜í–‰
    fit = model.fit()

    # íŒŒì´ì¬ ë¶„ì„ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥í•œë‹¤.
    summary = fit.summary()

    # ì²« ë²ˆì§¸, ì„¸ ë²ˆì§¸ í‘œì˜ ë‚´ìš©ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë¶„í•´
    my = {}

    for k in range(0, 3, 2):
        items = summary.tables[k].data
        # print(items)

        for item in items:
            # print(item)
            n = len(item)

            for i in range(0, n, 2):
                key = item[i].strip()[:-1]
                value = item[i+1].strip()

                if key and value:
                    my[key] = value

    # ë‘ ë²ˆì§¸ í‘œì˜ ë‚´ìš©ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë¶„í•´í•˜ì—¬ myì— ì¶”ê°€
    my['variables'] = []
    name_list = list(data.columns)
    #print(name_list)

    for i, v in enumerate(summary.tables[1].data):
        if i == 0:
            continue

        # ë³€ìˆ˜ì˜ ì´ë¦„
        name = v[0].strip()

        vif = 0

        # InterceptëŠ” ì œì™¸
        if name in name_list:
            # ë³€ìˆ˜ì˜ ì´ë¦„ ëª©ë¡ì—ì„œ í˜„ì¬ ë³€ìˆ˜ê°€ ëª‡ ë²ˆì§¸ í•­ëª©ì¸ì§€ ì°¾ê¸° 
            j = name_list.index(name)
            vif = variance_inflation_factor(data, j)

        my['variables'].append({
            "name": name,
            "coef": v[1].strip(),
            "std err": v[2].strip(),
            "t": v[3].strip(),
            "P-value": v[4].strip(),
            "Beta": 0,
            "VIF": vif,
        })

    # ê²°ê³¼í‘œë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ êµ¬ì„±
    mylist = []
    yname_list = []
    xname_list = []

    for i in my['variables']:
        if i['name'] == 'Intercept':
            continue

        yname_list.append(y)
        xname_list.append(i['name'])

        item = {
            "B": i['coef'],
            "í‘œì¤€ì˜¤ì°¨": i['std err'],
            "Î²": i['Beta'],
            "t": "%s*" % i['t'],
            "ìœ ì˜í™•ë¥ ": i['P-value'],
            "VIF": i["VIF"]
        }

        mylist.append(item)

    table = DataFrame(mylist,
                   index=MultiIndex.from_arrays([yname_list, xname_list], names=['ì¢…ì†ë³€ìˆ˜', 'ë…ë¦½ë³€ìˆ˜']))
    
    # ë¶„ì„ê²°ê³¼
    result = "ğ‘…(%s), ğ‘…^2(%s), ğ¹(%s), ìœ ì˜í™•ë¥ (%s), Durbin-Watson(%s)" % (my['R-squared'], my['Adj. R-squared'], my['F-statistic'], my['Prob (F-statistic)'], my['Durbin-Watson'])

    # ëª¨í˜• ì í•©ë„ ë³´ê³ 
    goodness = "%sì— ëŒ€í•˜ì—¬ %së¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ %s(F(%s,%s) = %s, p < 0.05)." % (y, ",".join(x), "ìœ ì˜í•˜ë‹¤" if float(my['Prob (F-statistic)']) < 0.05 else "ìœ ì˜í•˜ì§€ ì•Šë‹¤", my['Df Model'], my['Df Residuals'], my['F-statistic'])

    # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
    varstr = []

    for i, v in enumerate(my['variables']):
        if i == 0:
            continue
        
        s = "%sì˜ íšŒê·€ê³„ìˆ˜ëŠ” %s(p%s0.05)ë¡œ, %sì— ëŒ€í•˜ì—¬ %s."
        k = s % (v['name'], v['coef'], "<" if float(v['P-value']) < 0.05 else '>', y, 'ìœ ì˜ë¯¸í•œ ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤' if float(v['P-value']) < 0.05 else 'ìœ ì˜í•˜ì§€ ì•Šì€ ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤')

        varstr.append(k)

    # ë¦¬í„´í• 
    return (model, fit, summary, table, result, goodness, varstr)



class OlsResult:
    def __init__(self):
        self._model = None
        self._fit = None
        self._summary = None
        self._table = None
        self._result = None
        self._goodness = None
        self._varstr = None

    @property
    def model(self):
        """
        ë¶„ì„ëª¨ë¸
        """
        return self._model

    @model.setter
    def model(self, value):
        self._model = value

    @property
    def fit(self):
        """
        ë¶„ì„ê²°ê³¼ ê°ì²´
        """
        return self._fit

    @fit.setter
    def fit(self, value):
        self._fit = value

    @property
    def summary(self):
        """
        ë¶„ì„ê²°ê³¼ ìš”ì•½ ë³´ê³ 
        """
        return self._summary

    @summary.setter
    def summary(self, value):
        self._summary = value

    @property
    def table(self):
        """
        ê²°ê³¼í‘œ
        """
        return self._table

    @table.setter
    def table(self, value):
        self._table = value

    @property
    def result(self):
        """
        ê²°ê³¼í‘œ ë¶€ê°€ ì„¤ëª…
        """
        return self._result

    @result.setter
    def result(self, value):
        self._result = value

    @property
    def goodness(self):
        """
        ëª¨í˜• ì í•©ë„ ë³´ê³ 
        """
        return self._goodness

    @goodness.setter
    def goodness(self, value):
        self._goodness = value

    @property
    def varstr(self):
        """
        ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
        """
        return self._varstr

    @varstr.setter
    def varstr(self, value):
        self._varstr = value


def my_ols(data, y, x):
    model, fit, summary, table, result, goodness, varstr = ext_ols(data, y, x)

    ols_result = OlsResult()
    ols_result.model = model
    ols_result.fit = fit
    ols_result.summary = summary
    ols_result.table = table
    ols_result.result = result
    ols_result.goodness = goodness
    ols_result.varstr = varstr

    return ols_result

def scalling(df, yname):
    """
    ë°ì´í„° í”„ë ˆì„ì„ í‘œì¤€í™” í•œë‹¤.

    Parameters
    -------
    - df: ë°ì´í„° í”„ë ˆì„
    - yname: ì¢…ì†ë³€ìˆ˜ ì´ë¦„

    Returns
    -------
    - x_train_std_df: í‘œì¤€í™”ëœ ë…ë¦½ë³€ìˆ˜ ë°ì´í„° í”„ë ˆì„
    - y_train_std_df: í‘œì¤€í™”ëœ ì¢…ì†ë³€ìˆ˜ ë°ì´í„° í”„ë ˆì„
    """
    x_train = df.drop([yname], axis=1)
    x_train_std = StandardScaler().fit_transform(x_train)
    x_train_std_df = DataFrame(x_train_std, columns=x_train.columns)
    
    y_train = df.filter([yname])
    y_train_std = StandardScaler().fit_transform(y_train)
    y_train_std_df = DataFrame(y_train_std, columns=y_train.columns)

    return (x_train_std_df, y_train_std_df)

def get_best_features(x_train_std_df):
    pca_model = pca()
    fit = pca_model.fit_transform(x_train_std_df)
    topfeat_df = fit['topfeat']
    
    best = topfeat_df.query("type=='best'")
    feature = list(set(list(best['feature'])))
    
    return (feature, topfeat_df)